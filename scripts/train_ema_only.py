#!/usr/bin/env python3
"""
EMAæ³¨æ„åŠ›ç‹¬ç«‹å®éªŒ - ä¿®æ­£ç‰ˆï¼ˆå…¼å®¹YOLOv8åŸç»“æ„ï¼‰
"""

import os
import sys
import torch
import torch.nn as nn
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from ultralytics import YOLO
from ultralytics.nn.modules import Conv, C2f, SPPF
from utils.path_manager import path_manager


# =====================================================
# ================  EMA æ³¨æ„åŠ›æ¨¡å—  ====================
# =====================================================
class EMAttention(nn.Module):
    """EMAæ³¨æ„åŠ›æœºåˆ¶"""

    def __init__(self, channels, reduction=16):
        super().__init__()
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))
        self.conv = nn.Conv2d(channels, channels, kernel_size=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        b, c, h, w = x.size()
        x_h = self.pool_h(x).expand(-1, -1, -1, w)
        x_w = self.pool_w(x).expand(-1, -1, h, -1)
        attn = self.sigmoid(self.conv(x_h * x_w))
        return x * attn


# =====================================================
# ================  EMA Bottleneck ====================
# =====================================================
class EMA_Bottleneck(nn.Module):
    """é›†æˆEMAæ³¨æ„åŠ›çš„Bottleneckæ¨¡å—ï¼ˆå…¼å®¹YOLOv8åŸç»“æ„å‚æ•° g è€Œé groupsï¼‰"""

    def __init__(self, c1, c2, shortcut=True, g=1):
        super().__init__()
        self.cv1 = Conv(c1, c2, 1, 1)
        self.cv2 = Conv(c2, c2, 3, 1, g=g)
        self.ema_attention = EMAttention(c2)
        self.add = shortcut and c1 == c2

    def forward(self, x):
        y = self.cv2(self.cv1(x))
        y = self.ema_attention(y)
        return x + y if self.add else y


# =====================================================
# ================  EMA_C2fæ¨¡å— =======================
# =====================================================
class EMA_C2f(nn.Module):
    """é›†æˆEMAæ³¨æ„åŠ›çš„C2fæ¨¡å—ï¼ˆä¿®æ­£ç‰ˆï¼‰"""

    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__()
        c_ = int(c2 * e)
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv((2 + n) * c_, c2, 1)
        self.m = nn.ModuleList(EMA_Bottleneck(c_, c_, shortcut, g=g) for _ in range(n))
        self.ema_attention = EMAttention(c2)

    def forward(self, x):
        y = [self.cv1(x)]
        for m in self.m:
            y.append(m(y[-1]))
        out = self.cv2(torch.cat(y, 1))
        out = self.ema_attention(out)
        return out


# =====================================================
# ================  EMAæ¨¡å‹é›†æˆå™¨  ====================
# =====================================================
class EMA_Model_Integrator:
    """EMAæ¨¡å‹é›†æˆå™¨ï¼ˆä¿®æ­£ç‰ˆï¼‰"""

    def __init__(self):
        self.target_layers = ['model.2', 'model.4', 'model.6', 'model.8']

    def integrate_ema_into_model(self, model):
        print("ğŸ”§ğŸ”§ å¼€å§‹é›†æˆEMAæ³¨æ„åŠ›åˆ°æ¨¡å‹ç»“æ„...")
        model_structure = model.model
        print("ğŸ“‹ğŸ“‹ æ¨¡å‹ç»“æ„åˆ†æ:")
        self._analyze_model_structure(model_structure)

        replaced_count = 0
        for layer_path in self.target_layers:
            if self._replace_c2f_with_ema_c2f(model_structure, layer_path):
                replaced_count += 1

        print(f"âœ… æˆåŠŸæ›¿æ¢äº† {replaced_count} ä¸ªC2fæ¨¡å—ä¸ºEMA_C2f")
        return model

    def _analyze_model_structure(self, model):
        print("ğŸ”ğŸ” åˆ†ææ¨¡å‹å±‚ç»“æ„...")
        model_to_analyze = model.model if hasattr(model, 'model') else model
        if not hasattr(model_to_analyze, '__iter__'):
            print("âš  æ¨¡å‹ç»“æ„ä¸å¯è¿­ä»£ï¼Œè·³è¿‡è¯¦ç»†åˆ†æ")
            return

        for i, module in enumerate(model_to_analyze):
            module_type = type(module).__name__
            print(f"å±‚ {i}: {module_type}")
            if isinstance(module, C2f):
                print(f"  â†’ C2fæ¨¡å—: è¾“å…¥={module.cv1.conv.in_channels}, è¾“å‡º={module.cv2.conv.out_channels}")

    def _replace_c2f_with_ema_c2f(self, model, layer_path):
        try:
            target_module = self._get_module_by_path(model, layer_path)
            if target_module is None:
                print(f"âš  æ‰¾ä¸åˆ°å±‚: {layer_path}")
                return False
            if not isinstance(target_module, C2f):
                print(f"âš  {layer_path} ä¸æ˜¯C2fæ¨¡å—ï¼Œå®é™…æ˜¯: {type(target_module).__name__}")
                return False

            # è·å–å‚æ•°
            c1 = target_module.cv1.conv.in_channels
            c2 = target_module.cv2.conv.out_channels
            n = len(target_module.m)
            shortcut = getattr(target_module.m[0], "add", False)
            g = getattr(target_module.m[0].cv2.conv, "groups", 1)
            e = getattr(target_module, "e", 0.5)

            ema_c2f = EMA_C2f(c1, c2, n=n, shortcut=shortcut, g=g, e=e)

            parent_module, module_name = self._get_parent_and_name(model, layer_path)
            if parent_module is not None:
                setattr(parent_module, module_name, ema_c2f)
                print(f"âœ… æ›¿æ¢ {layer_path} ä¸ºEMA_C2f (è¾“å…¥: {c1}, è¾“å‡º: {c2})")
                return True
            return False
        except Exception as e:
            print(f"âŒâŒ æ›¿æ¢ {layer_path} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _get_module_by_path(self, model, path):
        try:
            modules = path.split(".")
            current = model
            for m in modules:
                current = current[int(m)] if m.isdigit() else getattr(current, m)
            return current
        except Exception:
            return None

    def _get_parent_and_name(self, model, path):
        modules = path.split(".")
        parent_path = ".".join(modules[:-1])
        module_name = modules[-1]
        parent_module = self._get_module_by_path(model, parent_path)
        return parent_module, module_name


# =====================================================
# ================  å®éªŒä¸»ä½“é€»è¾‘ ======================
# =====================================================
class EMAOnlyExperiment:
    """EMAæ³¨æ„åŠ›ç‹¬ç«‹å®éªŒ"""

    def __init__(self):
        self.exp_name = "ema_only_fixed"
        self.description = "YOLOv8s + EMAæ³¨æ„åŠ›æœºåˆ¶"
        self.integrator = EMA_Model_Integrator()

    def run(self):
        print("=" * 60)
        print("       EMAæ³¨æ„åŠ›ç‹¬ç«‹å®éªŒï¼ˆä¿®æ­£ç‰ˆï¼‰")
        print("=" * 60)
        if not path_manager.validate_paths():
            print("âŒ ç¯å¢ƒéªŒè¯å¤±è´¥")
            return False

        exp_dir = path_manager.get_experiment_dir(self.exp_name)
        weights_file = exp_dir / "weights" / "best.pt"
        if weights_file.exists():
            print("âœ… å·²å­˜åœ¨è®­ç»ƒæƒé‡ï¼Œè·³è¿‡è®­ç»ƒ")
            return True

        print("ğŸ”„ åŠ è½½YOLOv8sæ¨¡å‹...")
        model = YOLO("yolov8s.pt")

        print("ğŸ”§ é›†æˆEMAæ¨¡å—...")
        model = self.integrator.integrate_ema_into_model(model)
        print("ğŸ“Š æ¨¡å‹ç»“æ„é›†æˆå®Œæ¯•")

        train_config = {
            'data': str(path_manager.dataset_config),
            'epochs': 50,
            'imgsz': 640,
            'batch': 8,
            'device': 'cuda' if torch.cuda.is_available() else 'cpu',
            'workers': 2,
            'project': str(path_manager.runs_dir),
            'name': self.exp_name,
            'exist_ok': True,
            'amp': False,
        }

        print("ğŸš€ å¼€å§‹è®­ç»ƒEMAæ¨¡å‹...")
        model.train(**train_config)
        print("âœ… EMAæ¨¡å‹è®­ç»ƒå®Œæˆ")
        return True


def main():
    experiment = EMAOnlyExperiment()
    experiment.run()


if __name__ == "__main__":
    main()
