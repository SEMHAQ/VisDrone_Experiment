#!/usr/bin/env python3
"""
éªŒè¯æ‰€æœ‰å®éªŒç»“æœçš„è„šæœ¬
"""

import json
import pandas as pd
from pathlib import Path
import sys

project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from ultralytics import YOLO
from utils.path_manager import path_manager


class ExperimentVerifier:
    """å®éªŒéªŒè¯å™¨"""

    def __init__(self):
        self.experiments = [
            'baseline',
            'ema_only',
            'bifpn_only',
            'full_model'
        ]

        self.results = {}

    def verify_all_experiments(self):
        """éªŒè¯æ‰€æœ‰å®éªŒ"""
        print("=" * 60)
        print("       å®éªŒéªŒè¯")
        print("=" * 60)

        for exp_name in self.experiments:
            print(f"\nğŸ” éªŒè¯å®éªŒ: {exp_name}")
            print("-" * 50)

            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
            exp_dir = path_manager.get_experiment_dir(exp_name)
            weights_file = exp_dir / "weights" / "best.pt"

            if not weights_file.exists():
                print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {weights_file}")
                self.results[exp_name] = {'status': 'missing', 'error': 'æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨'}
                continue

            # è¯„ä¼°æ¨¡å‹
            metrics = self.evaluate_model(exp_name, weights_file)

            if metrics:
                self.results[exp_name] = {
                    'status': 'completed',
                    'metrics': metrics
                }
                print(f"âœ… {exp_name} éªŒè¯å®Œæˆ")
            else:
                self.results[exp_name] = {
                    'status': 'failed',
                    'error': 'è¯„ä¼°å¤±è´¥'
                }
                print(f"âŒ {exp_name} éªŒè¯å¤±è´¥")

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()

        return self.results

    def evaluate_model(self, exp_name, weights_path):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        try:
            model = YOLO(str(weights_path))

            metrics = model.val(
                data=str(path_manager.dataset_config),
                split='val',
                imgsz=640,
                batch=16,
                conf=0.001,
                iou=0.6,
                device='cpu',
                verbose=False
            )

            return {
                'map50': float(metrics.box.map50),
                'map': float(metrics.box.map),
                'precision': float(metrics.box.p.mean()),
                'recall': float(metrics.box.r.mean()),
                'f1_score': self.calculate_f1_score(
                    float(metrics.box.p.mean()),
                    float(metrics.box.r.mean())
                )
            }

        except Exception as e:
            print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
            return None

    def calculate_f1_score(self, precision, recall):
        """è®¡ç®—F1åˆ†æ•°"""
        if precision + recall == 0:
            return 0.0
        return 2 * (precision * recall) / (precision + recall)

    def generate_report(self):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        print("\nğŸ“Š éªŒè¯ç»“æœæŠ¥å‘Š")
        print("=" * 60)

        # åˆ›å»ºç»“æœç›®å½•
        results_dir = Path("results")
        results_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆæ€§èƒ½å¯¹æ¯”è¡¨
        completed_exps = {k: v for k, v in self.results.items() if v['status'] == 'completed'}

        if completed_exps:
            df_data = []
            for exp_name, result in completed_exps.items():
                metrics = result['metrics']
                df_data.append({
                    'å®éªŒ': exp_name,
                    'mAP@0.5': metrics['map50'],
                    'mAP@0.5:0.95': metrics['map'],
                    'ç²¾ç¡®ç‡': metrics['precision'],
                    'å¬å›ç‡': metrics['recall'],
                    'F1åˆ†æ•°': metrics['f1_score']
                })

            df = pd.DataFrame(df_data)
            print("\nğŸ“ˆ æ€§èƒ½å¯¹æ¯”:")
            print(df.to_string(index=False, float_format='%.4f'))

            # ä¿å­˜CSV
            csv_path = results_dir / "performance_comparison.csv"
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"âœ… æ€§èƒ½å¯¹æ¯”è¡¨å·²ä¿å­˜: {csv_path}")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        json_path = results_dir / "verification_results.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜: {json_path}")

        # çŠ¶æ€æ€»ç»“
        print(f"\nğŸ“‹ å®éªŒçŠ¶æ€æ€»ç»“:")
        for exp_name, result in self.results.items():
            status_icon = "âœ…" if result['status'] == 'completed' else "âŒ"
            print(f"{status_icon} {exp_name}: {result['status']}")


def main():
    """ä¸»å‡½æ•°"""
    verifier = ExperimentVerifier()
    results = verifier.verify_all_experiments()

    print("\nğŸ¯ éªŒè¯å®Œæˆ!")
    return results


if __name__ == "__main__":
    main()