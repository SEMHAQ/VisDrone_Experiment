#!/usr/bin/env python3
"""
ä¿®å¤çš„ç»¼åˆè¯„ä¼°æ‰€æœ‰å®éªŒæ¨¡å‹
"""

import os
import sys
import json
import pandas as pd
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from ultralytics import YOLO
from utils.path_manager import path_manager


class FixedComprehensiveEvaluator:
    """ä¿®å¤çš„ç»¼åˆè¯„ä¼°å™¨"""

    def __init__(self):
        self.experiments = {
            'baseline': {
                'name': 'baseline',
                'description': 'åŸå§‹YOLOv8s',
                'weights_path': path_manager.get_experiment_dir('baseline') / 'weights' / 'best.pt'
            },
            'ema': {
                'name': 'ema',
                'description': 'YOLOv8s + EMAæ³¨æ„åŠ›',
                'weights_path': path_manager.get_experiment_dir('ema') / 'weights' / 'best.pt'
            },
            'bifpn': {
                'name': 'bifpn',
                'description': 'YOLOv8s + BiFPN',
                'weights_path': path_manager.get_experiment_dir('bifpn') / 'weights' / 'best.pt'
            },
            'full': {
                'name': 'full',
                'description': 'YOLOv8s + EMA + BiFPN',
                'weights_path': path_manager.get_experiment_dir('full') / 'weights' / 'best.pt'
            }
        }

        self.results = {}

    def check_model_files(self):
        """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        print("ğŸ” æ£€æŸ¥æ¨¡å‹æ–‡ä»¶...")
        missing_models = []

        for exp_name, config in self.experiments.items():
            weights_path = config['weights_path']
            if weights_path.exists():
                print(f"âœ… {exp_name}: {weights_path}")
            else:
                print(f"âŒ {exp_name}: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
                missing_models.append(exp_name)

        return missing_models

    def extract_metrics_from_output(self, output_text):
        """ä»è¯„ä¼°è¾“å‡ºæ–‡æœ¬ä¸­æå–æŒ‡æ ‡"""
        # è§£æè¯„ä¼°è¾“å‡ºæ–‡æœ¬ï¼Œæå–å…³é”®æŒ‡æ ‡
        lines = output_text.split('\n')
        metrics = {}

        for line in lines:
            if 'all' in line and 'Images' in line:
                # è§£æç±»ä¼¼: "all        548      38759      0.517      0.395      0.402      0.237"
                parts = line.split()
                if len(parts) >= 7:
                    try:
                        metrics['precision'] = float(parts[3])
                        metrics['recall'] = float(parts[4])
                        metrics['map50'] = float(parts[5])
                        metrics['map'] = float(parts[6])
                    except:
                        pass
            elif 'Speed:' in line:
                # è§£ææ¨ç†é€Ÿåº¦
                parts = line.split()
                for i, part in enumerate(parts):
                    if 'inference' in part and i > 0:
                        try:
                            metrics['inference_speed'] = float(parts[i - 1].replace('ms', ''))
                        except:
                            pass

        return metrics

    def evaluate_single_model(self, exp_name, config):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹ - å…¼å®¹ç‰ˆæœ¬"""
        print(f"\nğŸ“Š è¯„ä¼°æ¨¡å‹: {exp_name}")
        print(f"æè¿°: {config['description']}")
        print("-" * 50)

        weights_path = config['weights_path']

        if not weights_path.exists():
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {weights_path}")
            return None

        try:
            # åŠ è½½æ¨¡å‹
            model = YOLO(str(weights_path))

            # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
            print("å¼€å§‹è¯„ä¼°...")

            # ä½¿ç”¨æ›´å…¼å®¹çš„æ–¹å¼è·å–æŒ‡æ ‡
            import io
            import contextlib

            # æ•è·è¯„ä¼°è¾“å‡º
            output_capture = io.StringIO()
            with contextlib.redirect_stdout(output_capture):
                metrics = model.val(
                    data=str(path_manager.dataset_config),
                    split='val',
                    imgsz=640,
                    batch=16,
                    conf=0.001,
                    iou=0.6,
                    device='cpu',
                    verbose=True,  # è®¾ç½®ä¸ºTrueä»¥è·å–è¯¦ç»†è¾“å‡º
                    save_json=False  # æš‚æ—¶å…³é—­ä»¥é¿å…å¹²æ‰°
                )

            output_text = output_capture.getvalue()

            # ä»è¾“å‡ºæ–‡æœ¬ä¸­æå–æŒ‡æ ‡
            extracted_metrics = self.extract_metrics_from_output(output_text)

            # å°è¯•ä½¿ç”¨æ–°çš„API
            try:
                # ä½¿ç”¨æ–°çš„å±æ€§åç§°
                precision = metrics.box.p.mean() if hasattr(metrics.box, 'p') else extracted_metrics.get('precision', 0)
                recall = metrics.box.r.mean() if hasattr(metrics.box, 'r') else extracted_metrics.get('recall', 0)
                map50 = metrics.box.map50 if hasattr(metrics.box, 'map50') else extracted_metrics.get('map50', 0)
                map_val = metrics.box.map if hasattr(metrics.box, 'map') else extracted_metrics.get('map', 0)
            except:
                # å¦‚æœæ–°APIå¤±è´¥ï¼Œä½¿ç”¨æå–çš„æŒ‡æ ‡
                precision = extracted_metrics.get('precision', 0)
                recall = extracted_metrics.get('recall', 0)
                map50 = extracted_metrics.get('map50', 0)
                map_val = extracted_metrics.get('map', 0)

            result = {
                'map50': float(map50),
                'map': float(map_val),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': self.calculate_f1_score(float(precision), float(recall)),
                'inference_speed': extracted_metrics.get('inference_speed', 0)
            }

            print(f"âœ… {exp_name} è¯„ä¼°å®Œæˆ:")
            print(f"   mAP@0.5:     {result['map50']:.4f}")
            print(f"   mAP@0.5:0.95: {result['map']:.4f}")
            print(f"   ç²¾ç¡®ç‡:      {result['precision']:.4f}")
            print(f"   å¬å›ç‡:      {result['recall']:.4f}")
            print(f"   F1åˆ†æ•°:      {result['f1_score']:.4f}")

            return result

        except Exception as e:
            print(f"âŒ {exp_name} è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def calculate_f1_score(self, precision, recall):
        """è®¡ç®—F1åˆ†æ•°"""
        if precision + recall == 0:
            return 0.0
        return 2 * (precision * recall) / (precision + recall)

    def evaluate_all_models(self):
        """è¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
        print("ğŸ§ª å¼€å§‹ç»¼åˆè¯„ä¼°æ‰€æœ‰æ¨¡å‹")
        print("=" * 60)

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        missing_models = self.check_model_files()
        if missing_models:
            print(f"\nâš  ä»¥ä¸‹æ¨¡å‹æ–‡ä»¶ç¼ºå¤±: {missing_models}")
            print("è¯·å…ˆå®Œæˆè¿™äº›æ¨¡å‹çš„è®­ç»ƒ")
            return False

        # é€ä¸ªè¯„ä¼°æ¨¡å‹
        for exp_name, config in self.experiments.items():
            result = self.evaluate_single_model(exp_name, config)
            self.results[exp_name] = result

        # åˆ†æç»“æœ
        self.analyze_results()

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_comprehensive_report()

        return True

    def analyze_results(self):
        """åˆ†æè¯„ä¼°ç»“æœ"""
        print("\nğŸ“ˆ ç»“æœåˆ†æ")
        print("=" * 60)

        # è¿‡æ»¤æ‰è¯„ä¼°å¤±è´¥çš„æ¨¡å‹
        valid_results = {k: v for k, v in self.results.items() if v is not None}

        if not valid_results:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°ç»“æœ")
            return

        # åˆ›å»ºæ€§èƒ½å¯¹æ¯”è¡¨æ ¼
        df_data = []
        for exp_name, metrics in valid_results.items():
            row = {
                'å®éªŒ': exp_name,
                'æè¿°': self.experiments[exp_name]['description'],
                'mAP@0.5': metrics['map50'],
                'mAP@0.5:0.95': metrics['map'],
                'ç²¾ç¡®ç‡': metrics['precision'],
                'å¬å›ç‡': metrics['recall'],
                'F1åˆ†æ•°': metrics['f1_score'],
                'æ¨ç†é€Ÿåº¦(ms/img)': metrics['inference_speed']
            }
            df_data.append(row)

        df = pd.DataFrame(df_data)

        # æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”
        print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”è¡¨:")
        print("=" * 80)
        print(df.to_string(index=False, float_format='%.4f'))

        # è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”ï¼ˆç›¸å¯¹äºåŸºå‡†æ¨¡å‹ï¼‰
        if 'baseline' in valid_results:
            baseline_map50 = valid_results['baseline']['map50']
            baseline_map = valid_results['baseline']['map']
            baseline_f1 = valid_results['baseline']['f1_score']

            print(f"\nğŸ“ˆ ç›¸å¯¹äºåŸºå‡†æ¨¡å‹çš„æ”¹è¿›:")
            print("=" * 50)
            print("| å®éªŒ | mAP@0.5æ”¹è¿› | mAPæ”¹è¿› | F1æ”¹è¿› |")
            print("|------|------------|---------|--------|")

            for exp_name, metrics in valid_results.items():
                if exp_name != 'baseline':
                    map50_improvement = ((metrics['map50'] - baseline_map50) / baseline_map50) * 100
                    map_improvement = ((metrics['map'] - baseline_map) / baseline_map) * 100
                    f1_improvement = ((metrics['f1_score'] - baseline_f1) / baseline_f1) * 100

                    print(
                        f"| {exp_name} | {map50_improvement:+.2f}% | {map_improvement:+.2f}% | {f1_improvement:+.2f}% |")

        self.performance_df = df
        return df

    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        print("\nğŸ“„ ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š...")

        # åˆ›å»ºç»“æœç›®å½•
        results_dir = Path("results")
        results_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†ç»“æœ
        json_path = results_dir / "detailed_results.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜: {json_path}")

        # ä¿å­˜CSVæ ¼å¼çš„æ€§èƒ½å¯¹æ¯”
        if hasattr(self, 'performance_df'):
            csv_path = results_dir / "performance_comparison.csv"
            self.performance_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"âœ… æ€§èƒ½å¯¹æ¯”è¡¨å·²ä¿å­˜: {csv_path}")

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report(results_dir)

        print("ğŸ‰ ç»¼åˆè¯„ä¼°å®Œæˆ!")

    def generate_markdown_report(self, results_dir):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Š"""
        md_path = results_dir / "comprehensive_evaluation_report.md"

        with open(md_path, 'w', encoding='utf-8') as f:
            f.write("# VisDroneæ¶ˆèå®éªŒç»¼åˆè¯„ä¼°æŠ¥å‘Š\n\n")
            f.write("## å®éªŒæ¦‚è¿°\n\n")

            # å®éªŒåˆ—è¡¨
            f.write("| å®éªŒåç§° | æè¿° | çŠ¶æ€ |\n")
            f.write("|----------|------|------|\n")
            for exp_name, config in self.experiments.items():
                status = "âœ… å®Œæˆ" if self.results.get(exp_name) else "âŒ å¤±è´¥"
                f.write(f"| {exp_name} | {config['description']} | {status} |\n")

            f.write("\n## æ€§èƒ½å¯¹æ¯”\n\n")

            if hasattr(self, 'performance_df'):
                f.write("| å®éªŒ | mAP@0.5 | mAP@0.5:0.95 | ç²¾ç¡®ç‡ | å¬å›ç‡ | F1åˆ†æ•° | æ¨ç†é€Ÿåº¦(ms/img) |\n")
                f.write("|------|---------|--------------|--------|--------|--------|----------------|\n")

                for _, row in self.performance_df.iterrows():
                    f.write(f"| {row['å®éªŒ']} | {row['mAP@0.5']:.4f} | {row['mAP@0.5:0.95']:.4f} | "
                            f"{row['ç²¾ç¡®ç‡']:.4f} | {row['å¬å›ç‡']:.4f} | {row['F1åˆ†æ•°']:.4f} | {row['æ¨ç†é€Ÿåº¦(ms/img)']:.2f} |\n")

            # æ”¹è¿›åˆ†æ
            f.write("\n## æ”¹è¿›æ•ˆæœåˆ†æ\n\n")

            baseline_result = self.results.get('baseline')
            if baseline_result:
                f.write("### ç›¸å¯¹äºåŸºå‡†æ¨¡å‹çš„æ”¹è¿›ç™¾åˆ†æ¯”\n\n")
                f.write("| å®éªŒ | mAP@0.5æ”¹è¿› | mAP@0.5:0.95æ”¹è¿› | F1åˆ†æ•°æ”¹è¿› |\n")
                f.write("|------|------------|-----------------|-----------|\n")

                for exp_name, result in self.results.items():
                    if exp_name != 'baseline' and result:
                        map50_improvement = ((result['map50'] - baseline_result['map50']) / baseline_result[
                            'map50']) * 100
                        map_improvement = ((result['map'] - baseline_result['map']) / baseline_result['map']) * 100
                        f1_improvement = ((result['f1_score'] - baseline_result['f1_score']) / baseline_result[
                            'f1_score']) * 100

                        f.write(
                            f"| {exp_name} | {map50_improvement:+.2f}% | {map_improvement:+.2f}% | {f1_improvement:+.2f}% |\n")

            # ç»“è®º
            f.write("\n## ç»“è®º\n\n")

            # è‡ªåŠ¨ç”Ÿæˆç»“è®º
            best_model = None
            best_map50 = 0

            for exp_name, result in self.results.items():
                if result and result['map50'] > best_map50:
                    best_map50 = result['map50']
                    best_model = exp_name

            if best_model and baseline_result:
                improvement = ((best_map50 - baseline_result['map50']) / baseline_result['map50']) * 100
                f.write(f"- **æœ€ä½³æ¨¡å‹**: {best_model} (mAP@0.5: {best_map50:.4f})\n")
                f.write(f"- **ç›¸å¯¹äºåŸºå‡†æ¨¡å‹æå‡**: {improvement:+.2f}%\n")

            f.write("\n## å»ºè®®\n\n")
            f.write("1. æ ¹æ®å®éªŒç»“æœé€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹è¿›è¡Œéƒ¨ç½²\n")
            f.write("2. è€ƒè™‘æ¨¡å‹å¤æ‚åº¦å’Œæ¨ç†é€Ÿåº¦çš„å¹³è¡¡\n")
            f.write("3. è¿›ä¸€æ­¥ä¼˜åŒ–è¡¨ç°æœ€ä½³çš„æ”¹è¿›æ¨¡å—\n")

        print(f"âœ… MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {md_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("       ä¿®å¤çš„VisDroneæ¶ˆèå®éªŒç»¼åˆè¯„ä¼°")
    print("=" * 60)

    # éªŒè¯ç¯å¢ƒ
    if not path_manager.validate_paths():
        print("âŒ ç¯å¢ƒéªŒè¯å¤±è´¥")
        return

    evaluator = FixedComprehensiveEvaluator()
    success = evaluator.evaluate_all_models()

    if success:
        print("\nğŸ¯ è¯„ä¼°å®Œæˆ!")
        print("ğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨ results/ ç›®å½•")
    else:
        print("\nâŒ è¯„ä¼°å¤±è´¥")


if __name__ == "__main__":
    main()