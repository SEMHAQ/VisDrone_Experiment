#!/usr/bin/env python3
"""
BiFPN + EMAæ³¨æ„åŠ›è”åˆå®éªŒ - ä¿®æ­£ç‰ˆ
ç»“åˆBiFPNç‰¹å¾é‡‘å­—å¡”å’ŒEMAæ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŠ¿
"""

import os
import sys
import traceback
import torch
import torch.nn as nn
import torch.nn.functional as F
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from ultralytics import YOLO
from ultralytics.nn.modules import Conv, C2f, SPPF
from utils.path_manager import path_manager


class EMAttention(nn.Module):
    """EMAæ³¨æ„åŠ›æœºåˆ¶ï¼ˆä¿®æ­£ç‰ˆï¼‰"""

    def __init__(self, channels, reduction=16):
        super().__init__()
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))
        self.conv = nn.Conv2d(channels, channels, kernel_size=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        b, c, h, w = x.size()
        x_h = self.pool_h(x).expand(-1, -1, -1, w)
        x_w = self.pool_w(x).expand(-1, -1, h, -1)
        attn = self.sigmoid(self.conv(x_h * x_w))
        return x * attn


class EMA_Bottleneck(nn.Module):
    """é›†æˆEMAæ³¨æ„åŠ›çš„Bottleneckæ¨¡å—ï¼ˆä¿®æ­£ç‰ˆï¼‰"""

    def __init__(self, c1, c2, shortcut=True, g=1):
        super().__init__()
        self.cv1 = Conv(c1, c2, 1, 1)
        self.cv2 = Conv(c2, c2, 3, 1, g=g)
        self.ema_attention = EMAttention(c2)
        self.add = shortcut and c1 == c2

    def forward(self, x):
        y = self.cv2(self.cv1(x))
        y = self.ema_attention(y)
        return x + y if self.add else y


class EMA_C2f(nn.Module):
    """é›†æˆEMAæ³¨æ„åŠ›çš„C2fæ¨¡å—ï¼ˆä¿®æ­£ç‰ˆï¼Œå…¼å®¹YOLOv8ç»“æ„ï¼‰"""

    def __init__(self, c1, c2, n=1, shortcut=False, g=1, e=0.5):
        super().__init__()
        c_ = int(c2 * e)
        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv((2 + n) * c_, c2, 1)
        self.m = nn.ModuleList(EMA_Bottleneck(c_, c_, shortcut, g=g) for _ in range(n))
        self.ema_attention = EMAttention(c2)

    def forward(self, x):
        y = [self.cv1(x)]
        for m in self.m:
            y.append(m(y[-1]))
        out = self.cv2(torch.cat(y, 1))
        out = self.ema_attention(out)
        return out



# === FIX ===
# æŠŠ Bottleneck çš„å‚æ•°ä» groups æ”¹ä¸º gï¼Œä¿æŒä¸ YOLOv8 æºç å‘½åä¸€è‡´ï¼Œé¿å…å…³é”®å­—å†²çª
class EMA_Bottleneck(nn.Module):
    """é›†æˆEMAæ³¨æ„åŠ›çš„Bottleneckæ¨¡å—ï¼ˆå…¼å®¹ YOLOv8 å‚æ•° gï¼‰"""

    def __init__(self, in_channels, out_channels, shortcut=True, g=1):
        super(EMA_Bottleneck, self).__init__()
        self.cv1 = Conv(in_channels, out_channels, 1, 1)
        # é€šè¿‡ g ä¼ é€’ group å‚æ•°ç»™åº•å±‚ Convï¼ˆultralytics.Conv æ”¯æŒ gï¼‰
        self.cv2 = Conv(out_channels, out_channels, 3, 1, g=g)
        self.ema_attention = EMAttention(out_channels)
        self.add = shortcut and in_channels == out_channels

    def forward(self, x):
        y = self.cv2(self.cv1(x))
        y = self.ema_attention(y)
        return x + y if self.add else y


# === FIX ===
# EMA_C2f ä¹Ÿä½¿ç”¨ g åŒæ­¥ groups è¡Œä¸ºï¼Œå¹¶ä¿ç•™ expansion å‚æ•°åä¸åŸ C2f å¯¹é½
class EMA_C2f(nn.Module):
    """é›†æˆEMAæ³¨æ„åŠ›çš„C2fæ¨¡å—ï¼ˆä¿®æ­£ç‰ˆï¼‰"""

    def __init__(self, in_channels, out_channels, n=1, shortcut=False, g=1, expansion=0.5):
        super(EMA_C2f, self).__init__()

        # ä¿æŒåŸå§‹C2fç»“æ„
        hidden_dim = int(out_channels * expansion)
        self.conv1 = Conv(in_channels, hidden_dim, 1, 1)
        self.conv2 = Conv((2 + n) * hidden_dim, out_channels, 1)

        # åˆ›å»ºEMA Bottleneckåˆ—è¡¨ - ä½¿ç”¨ g ä¼ å‚
        self.m = nn.ModuleList(EMA_Bottleneck(hidden_dim, hidden_dim, shortcut, g=g) for _ in range(n))

        # åœ¨è¾“å‡ºå‰æ·»åŠ EMAæ³¨æ„åŠ›
        self.ema_attention = EMAttention(out_channels)

    def forward(self, x):
        # åŸå§‹C2få‰å‘ä¼ æ’­
        y = [self.conv1(x)]
        y.extend(m(y[-1]) for m in self.m)
        y.append(y[-1])  # åŸå§‹ç‰¹å¾

        # æ‹¼æ¥å¹¶å·ç§¯
        x_out = self.conv2(torch.cat(y, 1))

        # åº”ç”¨EMAæ³¨æ„åŠ›
        x_out = self.ema_attention(x_out)

        return x_out


class BiFPN_Module(nn.Module):
    """BiFPNç‰¹å¾é‡‘å­—å¡”æ¨¡å— - å®Œæ•´å®ç°"""

    def __init__(self, feature_channels=[256, 512, 1024], bifpn_channels=256):
        super(BiFPN_Module, self).__init__()

        self.feature_channels = feature_channels
        self.bifpn_channels = bifpn_channels
        self.num_levels = len(feature_channels)

        # è¾“å…¥æŠ•å½±å±‚
        self.input_proj = nn.ModuleList([
            nn.Conv2d(channels, bifpn_channels, 1) for channels in feature_channels
        ])

        # è‡ªä¸Šè€Œä¸‹è·¯å¾„ (Top-down path)
        self.top_down_convs = nn.ModuleList([
            nn.Sequential(
                Conv(bifpn_channels, bifpn_channels, 3, 1),
                nn.BatchNorm2d(bifpn_channels),
                nn.ReLU(inplace=True)
            ) for _ in range(self.num_levels - 1)
        ])

        # è‡ªä¸‹è€Œä¸Šè·¯å¾„ (Bottom-up path)
        self.bottom_up_convs = nn.ModuleList([
            nn.Sequential(
                Conv(bifpn_channels, bifpn_channels, 3, 1),
                nn.BatchNorm2d(bifpn_channels),
                nn.ReLU(inplace=True)
            ) for _ in range(self.num_levels - 1)
        ])

        # è¾“å‡ºæŠ•å½±å±‚
        self.output_proj = nn.ModuleList([
            nn.Conv2d(bifpn_channels, channels, 1) for channels in feature_channels
        ])

        # å¯å­¦ä¹ çš„æƒé‡å‚æ•°
        self.top_down_weights = nn.ParameterList([
            nn.Parameter(torch.ones(2, dtype=torch.float32)) for _ in range(self.num_levels - 1)
        ])

        self.bottom_up_weights = nn.ParameterList([
            nn.Parameter(torch.ones(2, dtype=torch.float32)) for _ in range(self.num_levels - 1)
        ])

        self.epsilon = 1e-4

    def weighted_fusion(self, features, weights):
        """åŠ æƒç‰¹å¾èåˆ"""
        normalized_weights = F.relu(weights)
        weight_sum = torch.sum(normalized_weights) + self.epsilon
        return sum(w / weight_sum * f for w, f in zip(normalized_weights, features))

    def forward(self, features):
        """
        å‰å‘ä¼ æ’­
        features: å¤šå°ºåº¦ç‰¹å¾åˆ—è¡¨ [P3, P4, P5]
        """
        # è¾“å…¥æŠ•å½±
        proj_features = []
        for i, feat in enumerate(features):
            proj_features.append(self.input_proj[i](feat))

        # è‡ªä¸Šè€Œä¸‹è·¯å¾„
        top_down_features = [proj_features[-1]]  # ä»æœ€é«˜å±‚å¼€å§‹

        for i in range(self.num_levels - 2, -1, -1):
            # ä¸Šé‡‡æ ·å¹¶èåˆ
            upsampled = F.interpolate(
                top_down_features[-1],
                scale_factor=2,
                mode='nearest'
            )

            fused = self.weighted_fusion(
                [proj_features[i], upsampled],
                self.top_down_weights[i]
            )

            top_down_features.append(self.top_down_convs[i](fused))

        # åè½¬é¡ºåºä»¥åŒ¹é…åŸå§‹å±‚çº§
        top_down_features = list(reversed(top_down_features))

        # è‡ªä¸‹è€Œä¸Šè·¯å¾„
        bottom_up_features = [top_down_features[0]]  # ä»æœ€åº•å±‚å¼€å§‹

        for i in range(1, self.num_levels):
            # ä¸‹é‡‡æ ·å¹¶èåˆ
            downsampled = F.avg_pool2d(
                bottom_up_features[-1],
                kernel_size=3,
                stride=2,
                padding=1
            )

            fused = self.weighted_fusion(
                [top_down_features[i], downsampled],
                self.bottom_up_weights[i - 1]
            )

            bottom_up_features.append(self.bottom_up_convs[i - 1](fused))

        # è¾“å‡ºæŠ•å½±
        output_features = []
        for i, feat in enumerate(bottom_up_features):
            output_features.append(self.output_proj[i](feat) + features[i])  # æ®‹å·®è¿æ¥

        return output_features


class BiFPN_EMA_Model_Integrator:
    """BiFPN + EMAæ¨¡å‹é›†æˆå™¨"""

    def __init__(self):
        # EMAæ›¿æ¢çš„ç›®æ ‡å±‚ï¼ˆä¿æŒä½ åŸæ¥çš„å±‚è·¯å¾„ï¼‰
        self.ema_target_layers = {
            'backbone': ['model.2', 'model.4', 'model.6', 'model.8'],
            'neck': []  # æš‚ä¸æ›¿æ¢
        }

        # BiFPNç‰¹å¾å±‚
        self.bifpn_feature_layers = ['model.4', 'model.6', 'model.9']
        self.detect_layer = 'model.22'

    def integrate_improvements_into_model(self, model):
        """å°†BiFPNå’ŒEMAæ”¹è¿›é›†æˆåˆ°æ¨¡å‹ä¸­"""
        print("ğŸ”§ğŸ”§ å¼€å§‹é›†æˆBiFPN + EMAæ”¹è¿›åˆ°æ¨¡å‹ç»“æ„...")

        # è·å–æ¨¡å‹ç»“æ„
        model_structure = model.model

        # ç¬¬ä¸€æ­¥ï¼šé›†æˆEMAæ³¨æ„åŠ›
        print("ğŸ”„ğŸ”„ é›†æˆEMAæ³¨æ„åŠ›æœºåˆ¶...")
        ema_replaced_count = self._integrate_ema_attention(model_structure)
        print(f"âœ… æˆåŠŸæ›¿æ¢äº† {ema_replaced_count} ä¸ªC2fæ¨¡å—ä¸ºEMA_C2f")

        # ç¬¬äºŒæ­¥ï¼šé›†æˆBiFPNç‰¹å¾é‡‘å­—å¡”
        print("ğŸ”„ğŸ”„ é›†æˆBiFPNç‰¹å¾é‡‘å­—å¡”...")
        bifpn_success = self._integrate_bifpn_module(model_structure)

        if bifpn_success:
            print("âœ… BiFPNé›†æˆæˆåŠŸ")
        else:
            print("âŒâŒ BiFPNé›†æˆå¤±è´¥")

        return model

    def _integrate_ema_attention(self, model):
        """é›†æˆEMAæ³¨æ„åŠ›æœºåˆ¶"""
        replaced_count = 0

        for layer_path in self.ema_target_layers['backbone']:
            if self._replace_c2f_with_ema_c2f(model, layer_path):
                replaced_count += 1

        for layer_path in self.ema_target_layers['neck']:
            if self._replace_c2f_with_ema_c2f(model, layer_path):
                replaced_count += 1

        return replaced_count

    def _replace_c2f_with_ema_c2f(self, model, layer_path):
        """å°†æŒ‡å®šè·¯å¾„çš„C2fæ¨¡å—æ›¿æ¢ä¸ºEMA_C2f"""
        try:
            # è·å–ç›®æ ‡æ¨¡å—
            target_module = self._get_module_by_path(model, layer_path)
            if target_module is None:
                print(f"âš  æ‰¾ä¸åˆ°å±‚: {layer_path}")
                return False

            # æ£€æŸ¥æ˜¯å¦æ˜¯C2fæ¨¡å—
            if not isinstance(target_module, C2f):
                print(f"âš  {layer_path} ä¸æ˜¯C2fæ¨¡å—ï¼Œè·³è¿‡")
                return False

            # è·å–æ¨¡å—å‚æ•°
            in_channels = target_module.cv1.conv.in_channels
            out_channels = target_module.cv2.conv.out_channels

            # è·å–å…¶ä»–å‚æ•°
            n = len(target_module.m) if hasattr(target_module, 'm') else 1
            # shortcut å­˜åœ¨äº Bottleneck çš„ add å±æ€§ä¸Šï¼ˆè‹¥ä¸å­˜åœ¨åˆ™å– Falseï¼‰
            shortcut = False
            try:
                if hasattr(target_module, 'm') and len(target_module.m) > 0:
                    shortcut = getattr(target_module.m[0], 'add', False)
            except Exception:
                shortcut = False

            # === å…¼å®¹è¯»å– groups/g å‚æ•° ===
            g = 1
            try:
                if hasattr(target_module, 'm') and len(target_module.m) > 0:
                    m0 = target_module.m[0]
                    # æ·±å…¥å°è¯•è¯»å–åº•å±‚ conv çš„ groups å±æ€§ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if hasattr(m0, 'cv2') and hasattr(m0.cv2, 'conv'):
                        g = getattr(m0.cv2.conv, 'groups', 1)
                    else:
                        g = 1
            except Exception:
                g = 1

            # expansionï¼ˆeï¼‰å°½é‡ä»åŸæ¨¡å—è¯»å–ï¼ˆè‹¥æ— åˆ™ä½¿ç”¨ 0.5ï¼‰
            expansion = getattr(target_module, 'e', 0.5)

            # åˆ›å»ºEMA_C2fæ¨¡å—ï¼ˆä½¿ç”¨ g è€Œä¸æ˜¯ groupsï¼‰
            ema_c2f = EMA_C2f(in_channels, out_channels, n=n, shortcut=shortcut, g=g, expansion=expansion)

            # æ›¿æ¢æ¨¡å—
            parent_module, module_name = self._get_parent_and_name(model, layer_path)
            if parent_module is not None:
                setattr(parent_module, module_name, ema_c2f)
                print(f"âœ… æ›¿æ¢ {layer_path} ä¸ºEMA_C2f (è¾“å…¥: {in_channels}, è¾“å‡º: {out_channels}, n={n}, g={g})")
                return True
            else:
                print(f"âŒâŒ æ— æ³•æ›¿æ¢ {layer_path}ï¼ˆæœªæ‰¾åˆ°çˆ¶æ¨¡å—ï¼‰")
                return False

        except Exception as e:
            print(f"âŒâŒ æ›¿æ¢ {layer_path} å¤±è´¥: {e}")
            traceback.print_exc()
            return False

    def _integrate_bifpn_module(self, model):
        """é›†æˆBiFPNæ¨¡å—"""
        try:
            # è·å–ç‰¹å¾é€šé“æ•°
            feature_channels = self._get_feature_channels(model)
            print(f"ğŸ“ŠğŸ“Š ç‰¹å¾é€šé“æ•°: {feature_channels}")

            # åˆ›å»ºBiFPNæ¨¡å—
            bifpn_module = BiFPN_Module(feature_channels)

            # æ›¿æ¢Detectå±‚çš„forwardæ–¹æ³•
            return self._replace_detect_forward(model, bifpn_module)

        except Exception as e:
            print(f"âŒâŒ BiFPNé›†æˆå¤±è´¥: {e}")
            traceback.print_exc()
            return False

    def _get_feature_channels(self, model):
        """è·å–ç‰¹å¾é€šé“æ•°"""
        feature_channels = []

        for layer_path in self.bifpn_feature_layers:
            module = self._get_module_by_path(model, layer_path)
            if module is not None:
                # è·å–è¾“å‡ºé€šé“æ•°
                try:
                    if hasattr(module, 'cv2') and hasattr(module.cv2, 'conv'):
                        channels = module.cv2.conv.out_channels
                    elif hasattr(module, 'conv') and hasattr(module.conv, 'out_channels'):
                        channels = module.conv.out_channels
                    else:
                        # é»˜è®¤å€¼
                        channels = 256
                except Exception:
                    channels = 256

                feature_channels.append(channels)
                print(f"âœ… {layer_path}: {channels} é€šé“")
            else:
                print(f"âš  æ‰¾ä¸åˆ°å±‚: {layer_path}")
                feature_channels.append(256)  # é»˜è®¤å€¼

        return feature_channels

    def _replace_detect_forward(self, model, bifpn_module):
        """æ›¿æ¢Detectå±‚çš„forwardæ–¹æ³•ä»¥é›†æˆBiFPN"""
        try:
            # æŸ¥æ‰¾Detectå±‚
            detect_module = self._get_module_by_path(model, self.detect_layer)
            if detect_module is None:
                print("âŒâŒ æ‰¾ä¸åˆ°Detectå±‚")
                return False

            # ä¿å­˜åŸå§‹forwardæ–¹æ³•
            original_forward = detect_module.forward

            # å®šä¹‰æ–°çš„forwardæ–¹æ³•
            def new_forward(self, x):
                # å¦‚æœè¾“å…¥æ˜¯å¤šå°ºåº¦ feature listï¼ˆP3,P4,P5ï¼‰ï¼Œåˆ™å…ˆç”¨ bifpn_module å¤„ç†
                if isinstance(x, (list, tuple)) and len(x) == 3:
                    bifpn_outputs = self.bifpn_module(x)
                    x = bifpn_outputs

                # è°ƒç”¨åŸå§‹ forwardï¼ˆä¿æŒåŸ Detect è¡Œä¸ºï¼‰
                return original_forward(x)

            # æ›¿æ¢ forward å¹¶é™„åŠ  bifpn_module
            detect_module.forward = new_forward.__get__(detect_module, type(detect_module))
            detect_module.bifpn_module = bifpn_module

            print("âœ… Detectå±‚forwardæ–¹æ³•æ›¿æ¢æˆåŠŸ")
            return True

        except Exception as e:
            print(f"âŒâŒ æ›¿æ¢Detectå±‚forwardæ–¹æ³•å¤±è´¥: {e}")
            traceback.print_exc()
            return False

    def _get_module_by_path(self, model, path):
        """æ ¹æ®è·¯å¾„è·å–æ¨¡å—"""
        try:
            modules = path.split('.')
            current_module = model

            for module_name in modules:
                if module_name.isdigit():
                    current_module = current_module[int(module_name)]
                else:
                    current_module = getattr(current_module, module_name)

            return current_module
        except (AttributeError, IndexError, KeyError):
            return None

    def _get_parent_and_name(self, model, path):
        """è·å–çˆ¶æ¨¡å—å’Œæ¨¡å—åç§°"""
        try:
            modules = path.split('.')
            if len(modules) == 1:
                return model, modules[0]

            parent_path = '.'.join(modules[:-1])
            module_name = modules[-1]

            parent_module = self._get_module_by_path(model, parent_path)
            return parent_module, module_name

        except Exception as e:
            print(f"âŒâŒ è·å–çˆ¶æ¨¡å—å¤±è´¥: {e}")
            traceback.print_exc()
            return None, None


class BiFPN_EMA_Experiment:
    """BiFPN + EMAè”åˆå®éªŒ"""

    def __init__(self):
        self.exp_name = "bifpn_ema_combined"
        self.description = "YOLOv8s + BiFPNç‰¹å¾é‡‘å­—å¡” + EMAæ³¨æ„åŠ›æœºåˆ¶"
        self.integrator = BiFPN_EMA_Model_Integrator()

    def run(self):
        """è¿è¡Œè”åˆå®éªŒ"""
        print("=" * 70)
        print("       BiFPN + EMAæ³¨æ„åŠ›è”åˆå®éªŒ")
        print("=" * 70)
        print(f"å®éªŒåç§°: {self.exp_name}")
        print(f"æè¿°: {self.description}")
        print("=" * 70)

        # éªŒè¯ç¯å¢ƒ
        if not path_manager.validate_paths():
            print("âŒâŒ ç¯å¢ƒéªŒè¯å¤±è´¥")
            return False

        # æ£€æŸ¥æ˜¯å¦å·²è®­ç»ƒ
        exp_dir = path_manager.get_experiment_dir(self.exp_name)
        weights_file = exp_dir / "weights" / "best.pt"

        if weights_file.exists():
            print("âœ… å®éªŒå·²å®Œæˆï¼Œè·³è¿‡è®­ç»ƒ")
            return True

        # åŠ è½½åŸºç¡€æ¨¡å‹
        print("ğŸ”„ğŸ”„ åŠ è½½YOLOv8sæ¨¡å‹...")
        model = YOLO('yolov8s.pt')

        # åº”ç”¨BiFPN + EMAæ”¹è¿›
        print("ğŸ”§ğŸ”§ åº”ç”¨BiFPN + EMAæ”¹è¿›...")
        model = self.apply_combined_improvements(model)

        # è®­ç»ƒé…ç½®
        train_config = {
            'data': str(path_manager.dataset_config),
            'epochs': 100,  # å¢åŠ è®­ç»ƒè½®æ•°ä»¥å……åˆ†å‘æŒ¥ç»„åˆä¼˜åŠ¿
            'imgsz': 640,
            'batch': 16,
            'patience': 25,
            'device': 'cuda' if torch.cuda.is_available() else 'cpu',
            'workers': 4,
            'project': str(path_manager.runs_dir),
            'name': self.exp_name,
            'exist_ok': True,
            'verbose': True,
            'save': True,
            'amp': True,  # å¼€å¯æ··åˆç²¾åº¦è®­ç»ƒ
            'lr0': 0.01,  # è°ƒæ•´å­¦ä¹ ç‡
            'lrf': 0.01,
            'momentum': 0.937,
            'weight_decay': 0.0005,
            'warmup_epochs': 3.0,
            'warmup_momentum': 0.8,
            'warmup_bias_lr': 0.1
        }

        # è®­ç»ƒæ¨¡å‹
        print("ğŸš€ğŸš€ å¼€å§‹è®­ç»ƒBiFPN + EMAç»„åˆæ¨¡å‹...")
        try:
            results = model.train(**train_config)
            print("âœ… BiFPN + EMAç»„åˆæ¨¡å‹è®­ç»ƒå®Œæˆ")
            return True
        except Exception as e:
            print(f"âŒâŒ ç»„åˆæ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
            traceback.print_exc()
            return False

    def apply_combined_improvements(self, model):
        """åº”ç”¨BiFPN + EMAç»„åˆæ”¹è¿›"""
        print("ğŸ”§ğŸ”§ å¼€å§‹é›†æˆBiFPNç‰¹å¾é‡‘å­—å¡”å’ŒEMAæ³¨æ„åŠ›æœºåˆ¶...")

        try:
            # ä½¿ç”¨é›†æˆå™¨ä¿®æ”¹æ¨¡å‹ç»“æ„
            model = self.integrator.integrate_improvements_into_model(model)

            # éªŒè¯æ¨¡å‹æ˜¯å¦è¢«ä¿®æ”¹
            improvements_applied = self._check_improvements_integration(model.model)

            if improvements_applied:
                print("âœ… BiFPN + EMAé›†æˆæˆåŠŸ")
            else:
                print("âŒâŒ BiFPN + EMAé›†æˆå¤±è´¥")

            return model

        except Exception as e:
            print(f"âŒâŒ ç»„åˆæ”¹è¿›é›†æˆå¤±è´¥: {e}")
            traceback.print_exc()
            return model  # è¿”å›åŸå§‹æ¨¡å‹ä½œä¸ºå¤‡é€‰

    def _check_improvements_integration(self, model):
        """æ£€æŸ¥æ”¹è¿›æ˜¯å¦æˆåŠŸé›†æˆ"""
        try:
            # æ£€æŸ¥EMAæ¨¡å—
            ema_count = 0
            for name, module in model.named_modules():
                if isinstance(module, (EMA_C2f, EMA_Bottleneck, EMAttention)):
                    ema_count += 1

            # æ£€æŸ¥BiFPNæ¨¡å—
            detect_module = self.integrator._get_module_by_path(model, self.integrator.detect_layer)
            bifpn_integrated = detect_module is not None and hasattr(detect_module, 'bifpn_module')

            print(f"ğŸ“ŠğŸ“Š é›†æˆæ£€æŸ¥ç»“æœ:")
            print(f"  EMAæ¨¡å—æ•°é‡: {ema_count}")
            print(f"  BiFPNé›†æˆçŠ¶æ€: {'âœ… æˆåŠŸ' if bifpn_integrated else 'âŒâŒ å¤±è´¥'}")

            return ema_count > 0 and bifpn_integrated

        except Exception:
            traceback.print_exc()
            return False

    def evaluate(self):
        """è¯„ä¼°ç»„åˆæ¨¡å‹"""
        print(f"\nğŸ“ŠğŸ“Š è¯„ä¼°BiFPN + EMAç»„åˆæ¨¡å‹...")

        exp_dir = path_manager.get_experiment_dir(self.exp_name)
        weights_file = exp_dir / "weights" / "best.pt"

        if not weights_file.exists():
            print("âŒâŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
            return None

        try:
            model = YOLO(str(weights_file))

            # è¯„ä¼°
            metrics = model.val(
                data=str(path_manager.dataset_config),
                split='val',
                imgsz=640,
                batch=16,
                conf=0.001,
                iou=0.6,
                device='cuda' if torch.cuda.is_available() else 'cpu',
                verbose=True
            )

            # æå–æŒ‡æ ‡
            result = {
                'map50': float(metrics.box.map50),
                'map': float(metrics.box.map),
                'precision': float(metrics.box.p.mean()),
                'recall': float(metrics.box.r.mean()),
                'f1_score': self.calculate_f1_score(
                    float(metrics.box.p.mean()),
                    float(metrics.box.r.mean())
                )
            }

            print(f"âœ… BiFPN + EMAç»„åˆæ¨¡å‹è¯„ä¼°å®Œæˆ:")
            print(f"   mAP@0.5: {result['map50']:.4f}")
            print(f"   mAP@0.5:0.95: {result['map']:.4f}")
            print(f"   ç²¾ç¡®ç‡: {result['precision']:.4f}")
            print(f"   å¬å›ç‡: {result['recall']:.4f}")
            print(f"   F1åˆ†æ•°: {result['f1_score']:.4f}")

            return result

        except Exception as e:
            print(f"âŒâŒ è¯„ä¼°å¤±è´¥: {e}")
            traceback.print_exc()
            return None

    def calculate_f1_score(self, precision, recall):
        """è®¡ç®—F1åˆ†æ•°"""
        if precision + recall == 0:
            return 0.0
        return 2 * (precision * recall) / (precision + recall)

    def diagnose_model(self):
        """è¯Šæ–­æ¨¡å‹ç»“æ„"""
        print(f"\nğŸ”ğŸ” è¯Šæ–­BiFPN + EMAç»„åˆæ¨¡å‹ç»“æ„...")

        exp_dir = path_manager.get_experiment_dir(self.exp_name)
        weights_file = exp_dir / "weights" / "best.pt"

        if not weights_file.exists():
            print("âŒâŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
            return

        try:
            model = YOLO(str(weights_file))

            # ç»Ÿè®¡EMAæ¨¡å—
            ema_count = 0
            ema_modules = []
            for name, module in model.model.named_modules():
                if isinstance(module, (EMA_C2f, EMA_Bottleneck, EMAttention)):
                    ema_count += 1
                    ema_modules.append(name)

            # æ£€æŸ¥BiFPNé›†æˆ
            detect_module = self.integrator._get_module_by_path(model.model, self.integrator.detect_layer)
            bifpn_integrated = detect_module is not None and hasattr(detect_module, 'bifpn_module')

            print(f"ğŸ“ŠğŸ“Š æ¨¡å‹è¯Šæ–­ç»“æœ:")
            print(f"  EMAæ¨¡å—æ•°é‡: {ema_count}")
            print(f"  BiFPNé›†æˆçŠ¶æ€: {'âœ… æˆåŠŸ' if bifpn_integrated else 'âŒâŒ å¤±è´¥'}")

            if ema_count > 0:
                print(f"  EMAæ¨¡å—ä½ç½®:")
                for module_name in ema_modules[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"    - {module_name}")
                if len(ema_modules) > 5:
                    print(f"    ... è¿˜æœ‰ {len(ema_modules) - 5} ä¸ªæ¨¡å—")

            if bifpn_integrated:
                bifpn_module = detect_module.bifpn_module
                print(f"  BiFPNæ¨¡å—å‚æ•°: {sum(p.numel() for p in bifpn_module.parameters()):,}")
                print(f"  è¾“å…¥é€šé“: {bifpn_module.feature_channels}")
                print(f"  BiFPNé€šé“: {bifpn_module.bifpn_channels}")

            print("âœ… æ¨¡å‹è¯Šæ–­å®Œæˆ")

        except Exception as e:
            print(f"âŒâŒ è¯Šæ–­å¤±è´¥: {e}")
            traceback.print_exc()

    def compare_with_baseline(self):
        """ä¸åŸºçº¿æ¨¡å‹æ¯”è¾ƒæ€§èƒ½"""
        print(f"\nğŸ“ˆğŸ“ˆ ä¸åŸºçº¿æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ...")

        # è¿™é‡Œå¯ä»¥æ·»åŠ ä¸åŸå§‹YOLOv8sçš„æ€§èƒ½æ¯”è¾ƒé€»è¾‘
        # å®é™…å®ç°éœ€è¦åŠ è½½åŸºçº¿æ¨¡å‹å¹¶è¯„ä¼°

        print("ğŸ”œ æ€§èƒ½æ¯”è¾ƒåŠŸèƒ½å¾…å®ç°...")
        return None


def main():
    """ä¸»å‡½æ•°"""
    experiment = BiFPN_EMA_Experiment()

    # è¿è¡Œå®éªŒ
    success = experiment.run()

    if success:
        # è¯Šæ–­æ¨¡å‹ç»“æ„
        experiment.diagnose_model()

        # è¯„ä¼°æ¨¡å‹
        experiment.evaluate()

        # æ€§èƒ½æ¯”è¾ƒ
        experiment.compare_with_baseline()

        print("\nğŸ¯ğŸ¯ BiFPN + EMAè”åˆå®éªŒå®Œæˆ!")
        print("âœ¨âœ¨ ç»„åˆæ¨¡å‹å……åˆ†åˆ©ç”¨äº†:")
        print("   âœ… BiFPNçš„å¤šå°ºåº¦ç‰¹å¾èåˆä¼˜åŠ¿")
        print("   âœ… EMAæ³¨æ„åŠ›çš„ç©ºé—´å…³ç³»å»ºæ¨¡èƒ½åŠ›")
        print("   âœ… ä¸¤è€…çš„ååŒå¢å¼ºæ•ˆæœ")
    else:
        print("\nâŒâŒ BiFPN + EMAè”åˆå®éªŒå¤±è´¥")


if __name__ == "__main__":
    main()
