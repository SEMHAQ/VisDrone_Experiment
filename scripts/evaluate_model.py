#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°è„šæœ¬
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(project_root)

from ultralytics import YOLO
from utils.path_manager import path_manager


def find_latest_model(exp_name="baseline"):
    """æŸ¥æ‰¾æŒ‡å®šå®éªŒçš„æœ€æ–°æ¨¡å‹"""
    exp_dir = path_manager.get_experiment_dir(exp_name)
    weights_dir = exp_dir / "weights"

    if not weights_dir.exists():
        print(f"âŒ å®éªŒç›®å½•ä¸å­˜åœ¨: {weights_dir}")
        return None

    # æŸ¥æ‰¾æœ€ä½³æ¨¡å‹
    best_model = weights_dir / "best.pt"
    if best_model.exists():
        return best_model

    # æŸ¥æ‰¾æœ€åä¸€ä¸ªæ¨¡å‹
    last_model = weights_dir / "last.pt"
    if last_model.exists():
        return last_model

    print(f"âŒ åœ¨ {exp_name} ä¸­æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶")
    return None


def evaluate_model(exp_name="baseline"):
    """è¯„ä¼°æŒ‡å®šå®éªŒçš„æ¨¡å‹"""

    print(f"ğŸ“Š å¼€å§‹è¯„ä¼°æ¨¡å‹: {exp_name}")
    print("=" * 50)

    # æŸ¥æ‰¾æ¨¡å‹
    model_path = find_latest_model(exp_name)
    if model_path is None:
        return

    print(f"ğŸ” æ‰¾åˆ°æ¨¡å‹: {model_path}")

    # éªŒè¯é…ç½®æ–‡ä»¶
    if not path_manager.dataset_config.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {path_manager.dataset_config}")
        return

    # åŠ è½½æ¨¡å‹
    print("ğŸ”„ åŠ è½½æ¨¡å‹...")
    model = YOLO(str(model_path))

    # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    print("ğŸ§ª åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
    metrics = model.val(
        data=str(path_manager.dataset_config),
        split='val',
        imgsz=640,
        batch=16,
        conf=0.001,  # ä½ç½®ä¿¡åº¦é˜ˆå€¼ä»¥è¯„ä¼°å¬å›ç‡
        iou=0.6,
        device=0,
        verbose=True
    )

    # æ‰“å°ç»“æœ
    print(f"\nğŸ¯ {exp_name} æ¨¡å‹æ€§èƒ½:")
    print(f"   mAP@0.5:     {metrics.box.map50:.4f}")
    print(f"   mAP@0.5:0.95: {metrics.box.map:.4f}")
    print(f"   ç²¾ç¡®ç‡:      {metrics.box.precision.mean():.4f}")
    print(f"   å¬å›ç‡:      {metrics.box.recall.mean():.4f}")

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    results_dir = path_manager.get_experiment_dir(exp_name) / "metrics"
    results_dir.mkdir(parents=True, exist_ok=True)

    results_file = results_dir / "evaluation_results.txt"
    with open(results_file, 'w', encoding='utf-8') as f:
        f.write(f"{exp_name} æ¨¡å‹è¯„ä¼°ç»“æœ\n")
        f.write("=" * 40 + "\n")
        f.write(f"è¯„ä¼°æ—¶é—´: {metrics.speed['inference']:.2f} ms/img\n")
        f.write(f"mAP@0.5:     {metrics.box.map50:.4f}\n")
        f.write(f"mAP@0.5:0.95: {metrics.box.map:.4f}\n")
        f.write(f"ç²¾ç¡®ç‡:      {metrics.box.precision.mean():.4f}\n")
        f.write(f"å¬å›ç‡:      {metrics.box.recall.mean():.4f}\n")
        f.write(f"å‚æ•°æ•°é‡:    {metrics.model.nparams if hasattr(metrics, 'model') else 'N/A'}\n")

    print(f"âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    return metrics


def compare_models(model_paths, model_names):
    """æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½"""
    print("ğŸ“ˆ æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("=" * 50)

    results = {}
    for path, name in zip(model_paths, model_names):
        if path.exists():
            model = YOLO(str(path))
            metrics = model.val(
                data=str(path_manager.dataset_config),
                split='val',
                imgsz=640,
                batch=16,
                verbose=False
            )
            results[name] = {
                'mAP50': metrics.box.map50,
                'mAP': metrics.box.map,
                'precision': metrics.box.precision.mean(),
                'recall': metrics.box.recall.mean()
            }
            print(f"âœ… {name}: mAP50 = {metrics.box.map50:.4f}")
        else:
            print(f"âŒ {name}: æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")

    return results


if __name__ == "__main__":
    # é»˜è®¤è¯„ä¼°åŸºå‡†æ¨¡å‹
    evaluate_model("baseline")