import torch
import torch.nn as nn
from ultralytics import YOLO
import types
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


# é¦–å…ˆå®šä¹‰ EMAttention ç±»ï¼Œç¡®ä¿å®ƒåœ¨å…¨å±€ä½œç”¨åŸŸä¸­å¯ç”¨
class EMAttention(nn.Module):
    """EMAæ³¨æ„åŠ›æœºåˆ¶ - ç®€åŒ–ç‰ˆæœ¬"""

    def __init__(self, channels, reduction=16):
        # ä½¿ç”¨ super() è€Œä¸å¸¦ç±»å
        super().__init__()
        self.groups = max(1, channels // reduction)

        # æ± åŒ–å±‚
        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))
        self.pool_w = nn.AdaptiveAvgPool2d((1, None))

        # å·ç§¯å±‚
        self.conv = nn.Conv2d(channels, channels, kernel_size=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        b, c, h, w = x.size()

        # æ°´å¹³æ± åŒ–
        x_h = self.pool_h(x)  # [b, c, h, 1]
        # å‚ç›´æ± åŒ–
        x_w = self.pool_w(x)  # [b, c, 1, w]

        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        x_h_expanded = x_h.expand(-1, -1, -1, w)
        x_w_expanded = x_w.expand(-1, -1, h, -1)

        attention = self.sigmoid(self.conv(x_h_expanded * x_w_expanded))

        return x * attention


# å®šä¹‰ BiFPN_Module ç±»
class BiFPN_Module(nn.Module):
    """BiFPNæ¨¡å— - æ›¿æ¢YOLOv8çš„PANet"""

    def __init__(self, feature_channels=[256, 512, 1024], bifpn_channels=256):
        super().__init__()

        self.P3_channels = feature_channels[0]
        self.P4_channels = feature_channels[1]
        self.P5_channels = feature_channels[2]
        self.bifpn_channels = bifpn_channels

        # è¾“å…¥æŠ•å½±
        self.p3_proj = nn.Conv2d(self.P3_channels, bifpn_channels, 1)
        self.p4_proj = nn.Conv2d(self.P4_channels, bifpn_channels, 1)
        self.p5_proj = nn.Conv2d(self.P5_channels, bifpn_channels, 1)

        # è‡ªä¸Šè€Œä¸‹è·¯å¾„
        self.p4_td_conv = nn.Sequential(
            nn.Conv2d(bifpn_channels, bifpn_channels, 3, padding=1),
            nn.BatchNorm2d(bifpn_channels),
            nn.SiLU(inplace=True)
        )
        self.p3_td_conv = nn.Sequential(
            nn.Conv2d(bifpn_channels, bifpn_channels, 3, padding=1),
            nn.BatchNorm2d(bifpn_channels),
            nn.SiLU(inplace=True)
        )

        # è‡ªä¸‹è€Œä¸Šè·¯å¾„
        self.p4_bu_conv = nn.Sequential(
            nn.Conv2d(bifpn_channels, bifpn_channels, 3, padding=1),
            nn.BatchNorm2d(bifpn_channels),
            nn.SiLU(inplace=True)
        )
        self.p5_bu_conv = nn.Sequential(
            nn.Conv2d(bifpn_channels, bifpn_channels, 3, padding=1),
            nn.BatchNorm2d(bifpn_channels),
            nn.SiLU(inplace=True)
        )

        # è¾“å‡ºæŠ•å½±
        self.p3_out = nn.Conv2d(bifpn_channels, self.P3_channels, 1)
        self.p4_out = nn.Conv2d(bifpn_channels, self.P4_channels, 1)
        self.p5_out = nn.Conv2d(bifpn_channels, self.P5_channels, 1)

        # æƒé‡å‚æ•°
        self.p4_td_weight = nn.Parameter(torch.ones(2))
        self.p3_td_weight = nn.Parameter(torch.ones(2))
        self.p4_bu_weight = nn.Parameter(torch.ones(2))
        self.p5_bu_weight = nn.Parameter(torch.ones(2))

        self.epsilon = 1e-4

    def weighted_fusion(self, features, weights):
        """åŠ æƒç‰¹å¾èåˆ"""
        normalized_weights = F.relu(weights)
        weight_sum = torch.sum(normalized_weights) + self.epsilon
        return sum(w / weight_sum * f for w, f in zip(normalized_weights, features))

    def forward(self, inputs):
        """
        inputs: [P3, P4, P5] å¤šå°ºåº¦ç‰¹å¾
        """
        # è¾“å…¥æŠ•å½±
        p3_in = self.p3_proj(inputs[0])
        p4_in = self.p4_proj(inputs[1])
        p5_in = self.p5_proj(inputs[2])

        # è‡ªä¸Šè€Œä¸‹è·¯å¾„
        p5_td = p5_in
        p4_td = self.p4_td_conv(self.weighted_fusion(
            [p4_in, F.interpolate(p5_td, scale_factor=2, mode='nearest')],
            self.p4_td_weight
        ))
        p3_td = self.p3_td_conv(self.weighted_fusion(
            [p3_in, F.interpolate(p4_td, scale_factor=2, mode='nearest')],
            self.p3_td_weight
        ))

        # è‡ªä¸‹è€Œä¸Šè·¯å¾„
        p3_out = p3_td
        p4_out = self.p4_bu_conv(self.weighted_fusion(
            [p4_td, F.interpolate(p3_out, scale_factor=0.5, mode='nearest')],
            self.p4_bu_weight
        ))
        p5_out = self.p5_bu_conv(self.weighted_fusion(
            [p5_td, F.interpolate(p4_out, scale_factor=0.5, mode='nearest')],
            self.p5_bu_weight
        ))

        # è¾“å‡ºæŠ•å½±
        p3_final = self.p3_out(p3_out) + inputs[0]  # æ®‹å·®è¿æ¥
        p4_final = self.p4_out(p4_out) + inputs[1]
        p5_final = self.p5_out(p5_out) + inputs[2]

        return [p3_final, p4_final, p5_final]


class HookManager:
    """Hookç®¡ç†å™¨"""

    def __init__(self, model):
        self.model = model
        self.hooks = []
        self.original_methods = {}

    def register_hook(self, module, hook_type, hook_fn):
        """æ³¨å†Œhook"""
        if hook_type == 'forward':
            handle = module.register_forward_hook(hook_fn)
        elif hook_type == 'forward_pre':
            handle = module.register_forward_pre_hook(hook_fn)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„hookç±»å‹: {hook_type}")

        self.hooks.append(handle)
        return handle

    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰hook"""
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()


class YOLOv8HookIntegrator:
    """YOLOv8 Hooké›†æˆå™¨ - ç®€åŒ–ç‰ˆæœ¬"""

    def __init__(self, model_path='yolov8s.pt'):
        self.model = YOLO(model_path)
        self.hook_manager = HookManager(self.model.model)
        self.ema_hooks = []

    def integrate_ema_attention(self, target_layers=None, reduction=16):
        """é›†æˆEMAæ³¨æ„åŠ›æœºåˆ¶ - ä¿®å¤ç‰ˆæœ¬"""
        print("ğŸ”§ é›†æˆEMAæ³¨æ„åŠ›æœºåˆ¶...")

        if target_layers is None:
            # ä½¿ç”¨æ›´ç®€å•çš„å±‚è·¯å¾„
            target_layers = ['model.4', 'model.6', 'model.9']

        for layer_path in target_layers:
            try:
                module = self._get_module_by_path(layer_path)
                if module is None:
                    print(f"âš  æ‰¾ä¸åˆ°å±‚: {layer_path}")
                    continue

                # è·å–é€šé“æ•°
                if hasattr(module, 'cv2') and hasattr(module.cv2, 'conv'):
                    channels = module.cv2.conv.out_channels
                else:
                    # ä¼°è®¡é€šé“æ•°
                    channels = 256
                    print(f"âš  æ— æ³•ç¡®å®š {layer_path} çš„é€šé“æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼: {channels}")

                # åˆ›å»ºEMAæ³¨æ„åŠ›å®ä¾‹
                ema_layer = EMAttention(channels, reduction)

                # åˆ›å»ºhookå‡½æ•°
                def create_ema_hook(ema_instance):
                    def ema_hook(module, input, output):
                        return ema_instance(output)

                    return ema_hook

                # æ³¨å†Œhook
                hook_fn = create_ema_hook(ema_layer)
                handle = self.hook_manager.register_hook(module, 'forward', hook_fn)
                self.ema_hooks.append((layer_path, handle))

                print(f"âœ… åœ¨ {layer_path} æ·»åŠ EMAæ³¨æ„åŠ› (é€šé“æ•°: {channels})")

            except Exception as e:
                print(f"âŒ åœ¨ {layer_path} æ·»åŠ EMAå¤±è´¥: {e}")

        return len(self.ema_hooks)

    def integrate_bifpn(self, feature_layer_paths=None):
        """é›†æˆBiFPNç‰¹å¾é‡‘å­—å¡”"""
        print("ğŸ”§ é›†æˆBiFPNç‰¹å¾é‡‘å­—å¡”...")

        if feature_layer_paths is None:
            # é»˜è®¤ç‰¹å¾å±‚è·¯å¾„ (P3, P4, P5)
            feature_layer_paths = ['model.4', 'model.6', 'model.9']

        # è·å–ç‰¹å¾é€šé“æ•°
        feature_channels = []
        for path in feature_layer_paths:
            module = self._get_module_by_path(path)
            if module and hasattr(module, 'cv2'):
                feature_channels.append(module.cv2.conv.out_channels)
            else:
                feature_channels.append(256)  # é»˜è®¤å€¼

        # åˆ›å»ºBiFPNæ¨¡å—
        self.bifpn_module = BiFPN_Module(feature_channels)

        # æ³¨å†Œç‰¹å¾æ•è·hook
        for i, path in enumerate(feature_layer_paths):
            module = self._get_module_by_path(path)
            if module:
                def create_capture_hook(idx):
                    def capture_hook(module, input, output):
                        return output

                    return capture_hook

                self.hook_manager.register_hook(
                    module, 'forward', create_capture_hook(i)
                )

        print("âœ… BiFPNé›†æˆå®Œæˆ")
        return True

    def integrate_full_model(self):
        """é›†æˆå®Œæ•´æ¨¡å‹ï¼ˆEMA + BiFPNï¼‰"""
        print("ğŸ”§ é›†æˆå®Œæ•´æ¨¡å‹...")

        # é›†æˆEMA
        ema_count = self.integrate_ema_attention()
        print(f"âœ… æ·»åŠ äº† {ema_count} ä¸ªEMAæ³¨æ„åŠ›æ¨¡å—")

        # é›†æˆBiFPN
        bifpn_success = self.integrate_bifpn()
        print(f"âœ… BiFPNé›†æˆ: {bifpn_success}")

        return ema_count > 0 and bifpn_success

    def _get_module_by_path(self, path):
        """æ ¹æ®è·¯å¾„è·å–æ¨¡å—"""
        try:
            modules = path.split('.')
            current_module = self.model.model

            for module_name in modules:
                if module_name.isdigit():
                    current_module = current_module[int(module_name)]
                else:
                    current_module = getattr(current_module, module_name)

            return current_module
        except (AttributeError, IndexError, KeyError):
            return None

    def train(self, **kwargs):
        """è®­ç»ƒæ¨¡å‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒé›†æˆæ¨¡å‹...")
        try:
            results = self.model.train(**kwargs)
            print("âœ… è®­ç»ƒå®Œæˆ")
            return results
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            return None

    def cleanup(self):
        """æ¸…ç†hook"""
        self.hook_manager.remove_hooks()
        print("ğŸ§¹ æ¸…ç†hookå®Œæˆ")


def create_ema_model():
    """åˆ›å»ºé›†æˆEMAæ³¨æ„åŠ›çš„æ¨¡å‹"""
    integrator = YOLOv8HookIntegrator('yolov8s.pt')
    integrator.integrate_ema_attention()
    return integrator


def create_bifpn_model():
    """åˆ›å»ºé›†æˆBiFPNçš„æ¨¡å‹"""
    integrator = YOLOv8HookIntegrator('yolov8s.pt')
    integrator.integrate_bifpn()
    return integrator


def create_full_model():
    """åˆ›å»ºé›†æˆEMAå’ŒBiFPNçš„å®Œæ•´æ¨¡å‹"""
    integrator = YOLOv8HookIntegrator('yolov8s.pt')
    integrator.integrate_full_model()
    return integrator


# æµ‹è¯•å‡½æ•°
def test_hook_integration():
    """æµ‹è¯•Hooké›†æˆ"""
    print("ğŸ§ª æµ‹è¯•Hooké›†æˆ...")

    # æµ‹è¯•EMAé›†æˆ
    print("\n1. æµ‹è¯•EMAé›†æˆ:")
    ema_integrator = create_ema_model()
    print(f"EMA hooksæ•°é‡: {len(ema_integrator.ema_hooks)}")

    # æµ‹è¯•BiFPNé›†æˆ
    print("\n2. æµ‹è¯•BiFPNé›†æˆ:")
    bifpn_integrator = create_bifpn_model()
    print(f"BiFPNé›†æˆ: {bifpn_integrator.bifpn_module is not None}")

    # æµ‹è¯•å®Œæ•´é›†æˆ
    print("\n3. æµ‹è¯•å®Œæ•´é›†æˆ:")
    full_integrator = create_full_model()
    print(f"EMA hooksæ•°é‡: {len(full_integrator.ema_hooks)}")
    print(f"BiFPNé›†æˆ: {full_integrator.bifpn_module is not None}")

    # æ¸…ç†
    ema_integrator.cleanup()
    bifpn_integrator.cleanup()
    full_integrator.cleanup()

    print("âœ… Hooké›†æˆæµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    test_hook_integration()